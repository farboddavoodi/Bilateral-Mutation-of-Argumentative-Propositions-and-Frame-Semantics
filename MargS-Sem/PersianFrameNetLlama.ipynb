{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5HMFUDMv9qL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8X7a4Cpvxgt"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iTNZe29yjw7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git@nightly git+https://github.com/unslothai/unsloth-zoo.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_6Wh7D8Lw8m"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTmu0_PRv9uH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U transformers peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruFzPMB2y6XJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh7irNCFJBzi",
        "outputId": "d34b821c-d769-48a1-dac7-6dd83b156163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePHE1bKg50yU",
        "outputId": "de1e5430-0a2a-4590-bae0-0eb5d608a2f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['README.md', 'adapter_model.safetensors', 'tokenizer.json', 'tokenizer_config.json', 'adapter_config.json', 'special_tokens_map.json']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "lora_path = \"/content/drive/MyDrive/English_SRL_unsloth_Llama-3.2-3B-Instruct-bnb-4bit/lora/\"\n",
        "print(os.listdir(lora_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l7szMYFD8AF",
        "outputId": "e5edd653-3f84-4683-80c1-bbcfe397ea80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.5.1+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
            "    Python  3.11.11 (you have 3.11.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from unsloth import is_bfloat16_supported\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import pandas as pd\n",
        "import ast\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import Trainer, TrainingArguments, BatchEncoding#AdamW, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExLj7VvND79D"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "json_data = '/content/drive/MyDrive/davodi.json'\n",
        "persian_df = pd.read_json(json_data)\n",
        "persian_df = persian_df.drop(['_id', 'sentence', 'frame', 'lexicalUnit', 'status', 'issuer', 'is_active', 'createdAt', 'updatedAt', 'PId', 'lang', 'description', 'lexicalUnitHint', 'reviewer', 'lexicalUnitHelper', 'frameHelper', 'frameName', 'lexicalUnitName'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snOJ5yhBQIzx"
      },
      "outputs": [],
      "source": [
        "def framenet_tags(FN_tags):\n",
        "    framenet_pattern = []\n",
        "    for FN_tag in FN_tags:\n",
        "        tag_type = FN_tag.get('tagType')\n",
        "        if tag_type == 5:\n",
        "            element = FN_tag.get('element', {})\n",
        "            element_name = element.get('name', '')\n",
        "            framenet_pattern.append(element_name)\n",
        "        else:\n",
        "            framenet_pattern.append('O')\n",
        "    return framenet_pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHlhIPT8QI4U"
      },
      "outputs": [],
      "source": [
        "persian_df['frameNetTags'] = persian_df['frameNetTags'].apply(lambda FN_tags : framenet_tags(FN_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-UwLI3LckVW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "torch.cuda.empty_cache()\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ghf_2UWxcCkd"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "persian_df_tr, persian_df_temp = train_test_split(persian_df, random_state=seed, test_size=.08)\n",
        "persian_df_val, persian_df_te = train_test_split(persian_df_temp, random_state=seed, test_size=.05)\n",
        "del persian_df_temp\n",
        "persian_df_tr.reset_index(drop=True, inplace=True)\n",
        "persian_df_val.reset_index(drop=True, inplace=True)\n",
        "persian_df_te.reset_index(drop=True, inplace=True)\n",
        "persian_df_tr = Dataset.from_pandas(persian_df_tr)\n",
        "persian_df_val = Dataset.from_pandas(persian_df_val)\n",
        "persian_df_te = Dataset.from_pandas(persian_df_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw3dKfXadGXZ"
      },
      "outputs": [],
      "source": [
        "frame_prompt_template = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
        "Please generate FrameNet roles for the provided text.\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "You are an expert in the field of Semantic Role Labeling and lexical resources especially FrameNet.\n",
        "You know anything about how to label sentence tokens with FrameNet frame elements.\n",
        "Please use the following text:\"+ {}+\"Here are the frame elements you have to use for labeling:\" + {}+\"\n",
        "Your task is to generate frame elements for the provided text.\n",
        "The output should be a list of frame elements in a list format. IF the token do not have any frame element, put 'O'.\n",
        "Make sure that you do NOT use any elements other than the ones I provided in this prompt.\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "{}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244,
          "referenced_widgets": [
            "a23bf03d5544400181cd1e93b30e638f",
            "98a83043d9b1462faa38dd8e6b95d1c3",
            "363388f82e7d466095d5f29472b65a4b",
            "c4ae016b2fec4516bb7084dfa1778b64",
            "af96b7eedb784c7b9bbbc13deee0ae70",
            "26fcf6651c04477cbe25f84f5ed8c5c5",
            "5ded04b402124d10badc432e94ba7026",
            "eaac8706d5504da78a902f907c152df1",
            "78f0be60c37c49519bc62285b015e630",
            "b9a82c1a9650440491173760e77a5865",
            "02eb311c9ff64a6ea7cd40e623edd332",
            "d8ef9023ecce42cd830717998256429b",
            "d2fd0cb892b14046b0fd5ef889fd8891",
            "d56558eb7bca49b1bbef2dbe88a13766",
            "f29d4d29225a4352bf6af97a52af7313",
            "fd6736de515e42f38e0c2017589754be",
            "d0df2cda20954d37a95cce5f32d5a0e7",
            "3ba06fea03694d7ebfc9474ef5e66614",
            "d3f026d6dbcd4e9a9039d0f76ff332ce",
            "3f1fb9856f5c4385bddee47eb0d36345",
            "0d33027e8bd249199d01a098e0e2617a",
            "45df047ec17b4dfc84dd1ad588c424fa",
            "1868b3133ea94430859d2b2f6a6fd62f",
            "b515df34f825460496b1eb4c535fcde3",
            "dacd9f7992fc428fb78fa436c3e69bde",
            "374b1be3dde54eea88b49e01ffe856b4",
            "1ab32153679a4500aab0f8ca40bd1f27",
            "0d0941351c394ebf8be42b289b1ad83e",
            "302e37fccb7c4d948aeb122ab5e43b99",
            "f5926c4135714a4ab569ce4055b72aee",
            "be8a0751013f44d7a25aee37dbe34496",
            "70c121f565f44eb9b75266bf44868732",
            "4d145f7aaa9b47b9b3936d434e5b3d5c",
            "b366fb30a2104193a46718136a1d75e8",
            "9fc48f24c516433c8bd74ff453deab4a",
            "694863df39294f5eaaf6289fbf654d5a",
            "2531222b03ff4404a1f4c78a3c21e283",
            "01e0f2457de6486aabc371c3f801b734",
            "431030bf165341e1b6e2caa8cf057642",
            "69ccca5ac9fa4e439d79d056c7324340",
            "b3a287dc665c4b66acbadd0eb793fd0f",
            "aa61d811549847a9842a666377d88291",
            "dfe78ce134e64993b383bed2be683a13",
            "11cf3fb58e5c4de496c87b2654b34f3a",
            "e712f717c09847788bd602fe46217cce",
            "1ecd6406066949e3b019b3a5362050dc",
            "337bc87a9d6248e9a6e755357b8fe332",
            "3f04087df4f042f2bfc650c3d427c700",
            "08506a972e0748679e035cb088f3a8c3",
            "75d6a0e9e9bb4d63922f868ae90fa674",
            "0824d177e373423da0bea185a2fb6798",
            "fdc863b061b847269a119a5f8204b2e1",
            "375cf2d1ece445e3ac1a01f7732353ca",
            "e725fc347314480fb5b046c88a49454a",
            "bc1ce70e818a4259a10fa2b002b727f5",
            "1ccd813c0efe42c0975649db2c643338",
            "e92528db52c94d5c99caa7294947f307",
            "b683d4ab2d824aa5b91e743e99e56215",
            "624629a577c14c969941ab694dadb3ea",
            "ecdd45785bd34e68a55abcc1eb097772",
            "3775f7736be54a1d99d4b4876056fd1c",
            "ea00636140bb4bec9b9004d18423c861",
            "7def81872dfa4d72832545ef8da8451e",
            "ed3566e0f2094e6d95a52ead83ef10dd",
            "0bdca159d01f47ce88496c1bf3bef5ce",
            "1a3f679785944f0c96f44ebb74cda44d"
          ]
        },
        "id": "dM2xsAI7D8Dg",
        "outputId": "0566cb41-3aa3-407d-96a4-d31892b33f17"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a23bf03d5544400181cd1e93b30e638f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8ef9023ecce42cd830717998256429b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1868b3133ea94430859d2b2f6a6fd62f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b366fb30a2104193a46718136a1d75e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e712f717c09847788bd602fe46217cce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ccd813c0efe42c0975649db2c643338",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,293,760 || all params: 3,215,043,584 || trainable%: 0.0713\n",
            "âœ… DeepSeek LLM Loaded with LoRA and 4-bit Precision!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "#model_name = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\"\n",
        "model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_compute_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config,device_map=\"auto\")\n",
        "lora_config = LoraConfig(r=8,lora_alpha=32,target_modules=[\"q_proj\", \"v_proj\"],lora_dropout=0.05,bias=\"none\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "print(\"âœ… DeepSeek LLM Loaded with LoRA and 4-bit Precision!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299,
          "referenced_widgets": [
            "2c311e99cc7a44b7a86da78d033c1203",
            "9efaf1bb90fc4dc6ababc06e4c7577f2",
            "c257319e6452457b860093c5b3a4e9be",
            "497ddf37bfdf4cce894806f25b1a31be",
            "bafd5e6514394e3a94e912162d3e54fb",
            "442b3bd3428f47bc8912dbff97a6ca60",
            "3708383d9444449d8f46121f9e67a717",
            "d9c4d142a4e84cf092e8aae3c5102c3b",
            "fb1d310ce8784cf0b26cf2c266e6b8fb",
            "3b39cec9d3c648b8a1277b7ffc0af274",
            "4f24148b76064e7ab591acad8b8f254d",
            "4b98d952ecd94bb6a6872305597d7467",
            "351f9a03db084f7fae758f520945a0f3",
            "ae50495f88174221a31739c312230307",
            "f1b1e7b0104f422d9c72a3d2d6834847",
            "63134491ed8b4e949991e0a56387e377",
            "5558ebe59c1b40139bb42f0327ad2c14",
            "e5b62f2d9c534d3487d8eb47cf9a58fe",
            "ee304b71ba44458a9aa3c2b2481842c3",
            "48ad2b0b95194a3790e9cef8ae28aeb8",
            "9e4a1f5c14354c79839d2e367d96f7fb",
            "ddf52a8ad0ed4ac49d87c24a760623b1",
            "45e58595a76e4f6bab81b8f9d7cd9763",
            "9bea4a5682fc43708c69b8772c80f644",
            "372893dad8464d1a814cfcae72cc49db",
            "074ce4da7cf34326af9d51cf0e6567cf",
            "b4b2f0f8dc7447479ef924e48d30a7fa",
            "ec1406eaa81746a59e5b43288037e023",
            "445efa7f7043477e80fd208b5ee27d7b",
            "628559c4ce26461c955ada42cbdac68c",
            "46cbf43e12134473b7cac14e063df4d3",
            "21304470e7ce4787a384b650f488ff76",
            "aac83e93a8f9422ca57cfea46122029a",
            "8de2a103d9a94d9985ed273aad4e691c",
            "dbde2d233fae415ba407493495d553ce",
            "342d91584b6c42c78e9f1a403efebde4",
            "1b15f0edaac0465f8a8b579e930f8ac4",
            "4448bd63bb5849e5a1e1ce275d5cb628",
            "2ad8158f553e4b3eb41f0267cd4b6640",
            "d26d5a9bb8134b4cab17a5578e8511e1",
            "49637d894f7e484ca3e499b2c02a3e3d",
            "bf0239d617a64fbdb695f1a6abebc901",
            "efa9e9e227b042d1814ef45668813a76",
            "3e13c1a0dd23465cae289ac68b10677c",
            "55997c8c59724d04b3d8e68e1c2b67d1",
            "b00bec1bcc1242138d154003e5c6d8f7",
            "ed5f36ff4469492788c2336d7cbe52e1",
            "fca31943a762460aae13fd595a2b2c26",
            "10fc13e6aa8643408a4e194ae90b13f3",
            "e6a46c29ec9f4ff78c35b73f67f378bc",
            "b63ca5c34daa4fe38d324451003ab2cb",
            "0af8079428b3486c8c46c244d9331f57",
            "9cc2a1b8b48646b1bca83217906e91ea",
            "11ab366330cf482e9283b200047862b3",
            "6721a46419884e8b850b99ddd9056c46"
          ]
        },
        "id": "_MFN4TOXDA8r",
        "outputId": "482d840e-f865-4dd0-cdaa-23c583c536d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c311e99cc7a44b7a86da78d033c1203",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b98d952ecd94bb6a6872305597d7467",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45e58595a76e4f6bab81b8f9d7cd9763",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8de2a103d9a94d9985ed273aad4e691c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55997c8c59724d04b3d8e68e1c2b67d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.3.19 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(model_name = model_name, max_seq_length = 2048,dtype = None,load_in_4bit = True,)\n",
        "model = FastLanguageModel.get_peft_model(model,r=16,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\",],\n",
        "    lora_alpha=16,lora_dropout=0,bias=\"none\",use_gradient_checkpointing=\"unsloth\",random_state=3407,use_rslora=False,loftq_config=None,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6FsOt4HOibM"
      },
      "outputs": [],
      "source": [
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_function(examples):\n",
        "    words = examples[\"words\"]\n",
        "    frams = examples[\"frameNetTags\"]\n",
        "    texts = []\n",
        "    for word, fram in zip(words, frams):\n",
        "        text = frame_prompt_template.format(word, str(frame_roles), fram) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return tokenizer(examples[\"words\"],padding=True,truncation=True,return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfTmhsBtdU0e"
      },
      "outputs": [],
      "source": [
        "frame_roles = set()\n",
        "for _, row in persian_df.iterrows():\n",
        "    framenet_str = ast.literal_eval(str(row['frameNetTags']))\n",
        "    frame_roles.update(framenet_str)\n",
        "frame_roles = list(frame_roles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "38611303269c4a579b68125d7a5aff98",
            "9a1d99eb0285420a895cf90c3cab974d",
            "19ca70be42714c9b9d25b110b4ab5d71",
            "53bf91af2d4746b79c09f8468e01d6eb",
            "8efe07633c6640ed9fe31960e9b2bf77",
            "bba66dcd42da48debd84883916064397",
            "1c20fbd013544f4596d860c0bbd5b373",
            "66278265dfa64e2f8477f04912ef4179",
            "9736969d7db541b09539c36d6f27595f",
            "cbbc26cdcd80423e814bc6bed3fc4cee",
            "78954cff3ede4e87a9a41bc2f159c963",
            "7f4698a1bbf04faface90f60a772da2c",
            "a4a7f4fc1e94408bade57d8b9cef230e",
            "737c83fa95b248f6873e8db33d32e058",
            "e7aad1fd92844fd7b83f76b0a40f1658",
            "15b02ec387ff4edc8d8e14cf02af5302",
            "af8899fd25c247508902b4113de048a2",
            "8dbbc18b933f45569c492ddbf3f245d6",
            "3df3793085fe4014bf76a5f977862f88",
            "e873b39fc33f48e4ab3ad49c7c04830b",
            "8a5d3db0f06c4eb3a758ca51be1224d8",
            "10171b37bba14bc1b634678482822cff",
            "c6c3c5614f2b477db0351bc20c02ed8f",
            "0bd94afec1e44241836fef8faa9c9271",
            "fc7d30e696c9426687539e07c6c8216e",
            "d30fb7c53434446a9d7cad892b439d9b",
            "f86169884bf24fbaba873c7254da82f1",
            "a041b9c94d3e4861bee03661ef3704d3",
            "419d31e2d5cf4c949d103545f42a7537",
            "87316073f4474bdca0251acbfe14c43c",
            "65e61ac84a304866a63ad6ae8b7ce633",
            "3f65c5b4c04b4d04b1738133d0a99854",
            "54efadd410734fd08202b1f1083640d9"
          ]
        },
        "id": "frNuhGSDb28v",
        "outputId": "cdb97c2f-9d6f-4b4f-cc43-5eca78c498ef"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38611303269c4a579b68125d7a5aff98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/92 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f4698a1bbf04faface90f60a772da2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6c3c5614f2b477db0351bc20c02ed8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "main_dataset = persian_df_tr.map(formatting_prompts_function, remove_columns=persian_df_tr.column_names, batched=True)#, num_proc=1\n",
        "eval_dataset = persian_df_val.map(formatting_prompts_function, remove_columns=persian_df_val.column_names, batched=True)\n",
        "test_dataset = persian_df_te.map(formatting_prompts_function, remove_columns=persian_df_te.column_names, batched=True)\n",
        "\n",
        "main_dataset = main_dataset.shuffle(seed=seed)\n",
        "eval_dataset = eval_dataset.shuffle(seed=seed)\n",
        "test_dataset = test_dataset.shuffle(seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaFAUNippYMp",
        "outputId": "ff7a7297-059e-4028-a045-bfdca68b5bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: os.environ[CUDA_VISIBLE_DEVICES]: command not found\n"
          ]
        }
      ],
      "source": [
        "!os.environ['CUDA_VISIBLE_DEVICES'] ='0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQvF9J1ZvS7Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    labels = torch.tensor(labels, dtype=torch.int32)\n",
        "    print(\"Predictions:\", predictions.shape)\n",
        "    print(\"Labels:\", labels.shape)\n",
        "    results = {}\n",
        "    def compute_token_accuracy(predicted_ids, label_ids):\n",
        "        total_tokens = 0\n",
        "        correct_tokens = 0\n",
        "        for pred, ref in zip(predicted_ids, label_ids):\n",
        "            for p_token, r_token in zip(pred, ref):\n",
        "                if r_token != -100:\n",
        "                    total_tokens += 1\n",
        "                    if p_token == r_token:\n",
        "                        correct_tokens += 1\n",
        "        return (correct_tokens / total_tokens) * 100 if total_tokens > 0 else 0.0\n",
        "    if labels is not None:\n",
        "        logits_tensor = torch.tensor(logits, dtype=torch.float32)\n",
        "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "        per_sample_losses = []\n",
        "        for sample_logits, sample_labels in zip(logits_tensor, labels_tensor):\n",
        "            loss = F.cross_entropy(sample_logits.view(-1, sample_logits.size(-1)),sample_labels.view(-1),reduction=\"none\")\n",
        "            per_sample_loss = loss.view(sample_labels.size(0), -1).mean(dim=1).mean().item()\n",
        "            per_sample_losses.append(per_sample_loss)\n",
        "        results[\"manual_eval_loss\"] = sum(per_sample_losses) / len(per_sample_losses)\n",
        "        results[\"perplexity\"] = math.exp(results[\"manual_eval_loss\"])\n",
        "        results[\"loss_per_sample\"] = per_sample_losses\n",
        "\n",
        "    # 2. Token-Level Metrics\n",
        "    if labels is not None:\n",
        "        flattened_preds = []\n",
        "        flattened_labels = []\n",
        "        for pred, label in zip(predictions, labels):\n",
        "            for p_token, l_token in zip(pred, label):\n",
        "                if l_token != -100:\n",
        "                    flattened_preds.append(p_token)\n",
        "                    flattened_labels.append(l_token)\n",
        "\n",
        "        if flattened_labels and flattened_preds:\n",
        "            results[\"accuracy_score\"] = accuracy_score(flattened_labels, flattened_preds)\n",
        "            results[\"token_accuracy\"] = compute_token_accuracy(predictions, labels)\n",
        "            results[\"precision\"] = precision_score(flattened_labels, flattened_preds, average=\"macro\")\n",
        "            results[\"recall\"] = recall_score(flattened_labels, flattened_preds, average=\"macro\")\n",
        "            results[\"f1\"] = f1_score(flattened_labels, flattened_preds, average=\"macro\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8AJb-Yayxec"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=main_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=6,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=1,\n",
        "        learning_rate=2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=1,\n",
        "    ),\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE6DjpzU7FqJ"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_zHKpZb3KzN"
      },
      "outputs": [],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}